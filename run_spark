#!/usr/bin/env bash
#
# Spark does not reliably delete the bucket after dropping the table, so more
# force is used.
#

SPARK_CONF='--conf spark.sql.parquet.compression.codec=gzip'

# The INSERT contains a SELECT because hints are only supported for SELECT
# statements.
SQL_INSERT='INSERT INTO data SELECT /*+ REPARTITION(1) */ * FROM VALUES (1, 0), (2, 0), (3, 0), (4, 0);'
SQL_DELETE='DELETE FROM data WHERE key = 1;'
SQL_UPDATE='UPDATE data SET value = 1 WHERE key = 2;'
SQL_DROP_TABLE='DROP TABLE IF EXISTS data;'
SQL_DROP_SCHEMA="DROP SCHEMA IF EXISTS dml CASCADE;"
SQL_CREATE_SCHEMA="CREATE SCHEMA IF NOT EXISTS dml LOCATION 's3a://dml/';"

for VARIANT in hudi_cow_insert hudi_cow_bulk_insert hudi_mor_insert hudi_mor_bulk_insert iceberg_cow iceberg_mor delta_no_deletion_vectors delta_deletion_vectors; do

    if [[ "$VARIANT" == 'hudi_cow_insert' ]]; then
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT=hudi
        TABLE_PROPS="TBLPROPERTIES (type = 'cow', hoodie.spark.sql.insert.into.operation = 'insert')"
    elif [[ "$VARIANT" == 'hudi_cow_bulk_insert' ]]; then
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT=hudi
        TABLE_PROPS="TBLPROPERTIES (type = 'cow', hoodie.spark.sql.insert.into.operation = 'bulk_insert')"
    elif [[ "$VARIANT" == 'hudi_mor_insert' ]]; then
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT=hudi
        TABLE_PROPS="TBLPROPERTIES (type = 'mor', hoodie.spark.sql.insert.into.operation = 'insert')"
    elif [[ "$VARIANT" == 'hudi_mor_bulk_insert' ]]; then
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT=hudi
        TABLE_PROPS="TBLPROPERTIES (type = 'mor', hoodie.spark.sql.insert.into.operation = 'bulk_insert')"
    elif [[ "$VARIANT" == 'iceberg_cow' ]]; then
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT=iceberg
        TABLE_PROPS="TBLPROPERTIES (write.delete.mode = 'copy-on-write', write.update.mode = 'copy-on-write', write.merge.mode = 'copy-on-write', compression = 'gzip')"
    elif [[ "$VARIANT" == 'iceberg_mor' ]]; then
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT=iceberg
        TABLE_PROPS="TBLPROPERTIES (write.delete.mode = 'merge-on-read', write.update.mode = 'merge-on-read', write.merge.mode = 'merge-on-read', compression = 'gzip')"
    elif [[ "$VARIANT" == 'delta_no_deletion_vectors' ]]; then
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT=delta
        TABLE_PROPS=''
    elif [[ "$VARIANT" == 'delta_deletion_vectors' ]]; then
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT=delta
        TABLE_PROPS="TBLPROPERTIES (delta.enableDeletionVectors = true)"
    else
        OUTPUT_DIR="${VARIANT}/spark"
        FORMAT="${VARIANT}"
        TABLE_PROPS=''
    fi

    rm -rf "$OUTPUT_DIR"
    mkdir -p "$OUTPUT_DIR"

    SQL_CREATE_TABLE="CREATE TABLE data (key INT, value INT) USING $FORMAT $TABLE_PROPS;"

    # Setup
    spark-sql-hms-$FORMAT $SPARK_CONF -e "$SQL_DROP_SCHEMA"
    mcli mb -p minio/dml
    mcli rb --force minio/dml
    mcli mb minio/dml
    spark-sql-hms-$FORMAT $SPARK_CONF -e "$SQL_CREATE_SCHEMA"

    # CREATE
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_CREATE_TABLE"
    mcli cp -r "minio/dml/data/" "$OUTPUT_DIR/create"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_DROP_TABLE"
    mcli rm -r --force "minio/dml/data/"

    # INSERT
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_CREATE_TABLE"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_INSERT"
    mcli cp -r "minio/dml/data/" "$OUTPUT_DIR/insert"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_DROP_TABLE"
    mcli rm -r --force "minio/dml/data/"

    # INSERT 2x
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_CREATE_TABLE"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_INSERT"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_INSERT"
    mcli cp -r "minio/dml/data/" "$OUTPUT_DIR/insert2x"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_DROP_TABLE"
    mcli rm -r --force "minio/dml/data/"

    # DELETE
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_CREATE_TABLE"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_INSERT"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_DELETE"
    mcli cp -r "minio/dml/data/" "$OUTPUT_DIR/delete"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_DROP_TABLE"
    mcli rm -r --force "minio/dml/data/"

    # UPDATE
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_CREATE_TABLE"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_INSERT"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_UPDATE"
    mcli cp -r "minio/dml/data/" "$OUTPUT_DIR/update"
    spark-sql-hms-$FORMAT --database dml $SPARK_CONF -e "$SQL_DROP_TABLE"
    mcli rm -r --force "minio/dml/data/"

    # Cleanup
    spark-sql-hms-$FORMAT $SPARK_CONF -e "$SQL_DROP_SCHEMA"
    mcli rb --force minio/dml

done
